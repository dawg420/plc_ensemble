{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d23abc-2f2d-414f-b91a-798cfb59b3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 02:11:57.894370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749665517.905931 2888550 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749665517.909493 2888550 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749665517.918736 2888550 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749665517.918747 2888550 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749665517.918748 2888550 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749665517.918749 2888550 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-12 02:11:57.922217: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Model Training Pipeline\n",
      "==================================================\n",
      "Dataset: modbus_output_with_time.csv\n",
      "Max rows: 20000\n",
      "Train split: 0.95\n",
      "Save directory: models/saved_models\n",
      "\n",
      "==================== Training LSTM ====================\n",
      "Processing dataset for LSTM...\n",
      "  Processed 2000 transactions...\n",
      "  Processed 4000 transactions...\n",
      "  Processed 6000 transactions...\n",
      "  Processed 8000 transactions...\n",
      "  Processed 10000 transactions...\n",
      "  Processed 12000 transactions...\n",
      "  Processed 14000 transactions...\n",
      "  Processed 16000 transactions...\n",
      "  Processed 18000 transactions...\n"
     ]
    }
   ],
   "source": [
    "# train_and_save_models.py\n",
    "\"\"\"\n",
    "Script to train and save all ML models for the ensemble system\n",
    "Run this once to prepare all models before starting the ensemble\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from hmmlearn import hmm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Local imports\n",
    "from shared.utils import update_state_with_transaction, state_to_features, features_to_state\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===== BASE CLASSES FOR EACH MODEL =====\n",
    "\n",
    "class HMMStatePredictor:\n",
    "    \"\"\"HMM Model with temporal features\"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=25):\n",
    "        self.n_components = n_components\n",
    "        self.model = hmm.GaussianHMM(\n",
    "            n_components=n_components,\n",
    "            covariance_type=\"diag\",\n",
    "            n_iter=100,\n",
    "            tol=1e-3,\n",
    "            init_params=\"kmeans\",\n",
    "            algorithm=\"viterbi\",\n",
    "            random_state=42,\n",
    "            verbose=False\n",
    "        )\n",
    "        self.scaler = RobustScaler()\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def process_dataset(self, file_path, max_rows=20000):\n",
    "        \"\"\"Process dataset to extract state sequences\"\"\"\n",
    "        print(\"Processing dataset for HMM...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if len(df) > max_rows:\n",
    "            df = df.head(max_rows)\n",
    "        \n",
    "        holding_registers = {i: {\"value\": 0, \"last_changed\": 0, \"change_count\": 0} for i in range(39)}\n",
    "        coils = {i: {\"value\": 0, \"last_changed\": 0, \"change_count\": 0} for i in range(19)}\n",
    "        \n",
    "        state_sequence = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            current_state_features = state_to_features(holding_registers, coils, idx)\n",
    "            state_sequence.append(current_state_features)\n",
    "            update_state_with_transaction(row, idx, holding_registers, coils)\n",
    "            \n",
    "            if idx % 2000 == 0 and idx > 0:\n",
    "                print(f\"  Processed {idx} transactions...\")\n",
    "        \n",
    "        final_state_features = state_to_features(holding_registers, coils, len(df))\n",
    "        state_sequence.append(final_state_features)\n",
    "        \n",
    "        return np.array(state_sequence)\n",
    "    \n",
    "    def train(self, features, train_split=0.95):\n",
    "        \"\"\"Train the HMM\"\"\"\n",
    "        print(f\"Training HMM on {features.shape[0]} states...\")\n",
    "        \n",
    "        # Use only training data\n",
    "        train_size = int(len(features) * train_split)\n",
    "        train_features = features[:train_size]\n",
    "        \n",
    "        features_scaled = self.scaler.fit_transform(train_features)\n",
    "        self.model.fit(features_scaled)\n",
    "        self.is_fitted = True\n",
    "        print(f\"HMM training complete. Converged: {self.model.monitor_.converged}\")\n",
    "        return self\n",
    "    \n",
    "    def predict_next_state(self, current_state_dict, current_time=0):\n",
    "        \"\"\"Predict next state\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not trained!\")\n",
    "            \n",
    "        current_features = state_to_features(\n",
    "            current_state_dict[\"holding_registers\"], \n",
    "            current_state_dict[\"coils\"],\n",
    "            current_time\n",
    "        )\n",
    "        \n",
    "        current_features_scaled = self.scaler.transform(np.array(current_features).reshape(1, -1))\n",
    "        current_hidden_state = self.model.predict(current_features_scaled)[0]\n",
    "        \n",
    "        # Sample next state\n",
    "        next_hidden_state = np.random.choice(self.n_components, p=self.model.transmat_[current_hidden_state])\n",
    "        next_features_scaled = np.random.multivariate_normal(\n",
    "            self.model.means_[next_hidden_state],\n",
    "            self.model.covars_[next_hidden_state]\n",
    "        )\n",
    "        \n",
    "        next_features = self.scaler.inverse_transform(next_features_scaled.reshape(1, -1))[0]\n",
    "        next_holding_registers, next_coils = features_to_state(next_features)\n",
    "        \n",
    "        return {\"holding_registers\": next_holding_registers, \"coils\": next_coils}\n",
    "\n",
    "class LSTMStatePredictor:\n",
    "    \"\"\"LSTM Neural Network Model\"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length=10, lstm_units=128, dropout_rate=0.3):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.lstm_units = lstm_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.is_fitted = False\n",
    "        self.feature_dim = 174  # (39+19)*3\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Build LSTM model\"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(self.lstm_units, return_sequences=True, input_shape=(self.sequence_length, self.feature_dim)),\n",
    "            Dropout(self.dropout_rate),\n",
    "            LSTM(self.lstm_units // 2, return_sequences=False),\n",
    "            Dropout(self.dropout_rate),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(self.dropout_rate),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(self.feature_dim, activation='linear')\n",
    "        ])\n",
    "        \n",
    "        optimizer = Adam(learning_rate=0.0005, clipnorm=1.0)\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "    \n",
    "    def process_dataset(self, file_path, max_rows=20000):\n",
    "        \"\"\"Process dataset for LSTM\"\"\"\n",
    "        print(\"Processing dataset for LSTM...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if len(df) > max_rows:\n",
    "            df = df.head(max_rows)\n",
    "        \n",
    "        holding_registers = {i: {\"value\": 0, \"last_changed\": 0, \"change_count\": 0} for i in range(39)}\n",
    "        coils = {i: {\"value\": 0, \"last_changed\": 0, \"change_count\": 0} for i in range(19)}\n",
    "        \n",
    "        state_sequence = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            current_state_features = state_to_features(holding_registers, coils, idx)\n",
    "            state_sequence.append(current_state_features)\n",
    "            update_state_with_transaction(row, idx, holding_registers, coils)\n",
    "            \n",
    "            if idx % 2000 == 0 and idx > 0:\n",
    "                print(f\"  Processed {idx} transactions...\")\n",
    "        \n",
    "        final_state_features = state_to_features(holding_registers, coils, len(df))\n",
    "        state_sequence.append(final_state_features)\n",
    "        \n",
    "        return np.array(state_sequence)\n",
    "    \n",
    "    def create_sequences(self, features):\n",
    "        \"\"\"Create input-output sequences for LSTM training\"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        for i in range(self.sequence_length, len(features)):\n",
    "            X.append(features[i-self.sequence_length:i])\n",
    "            y.append(features[i])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def train(self, features, train_split=0.95):\n",
    "        \"\"\"Train LSTM\"\"\"\n",
    "        print(f\"Training LSTM on {features.shape[0]} states...\")\n",
    "        \n",
    "        # Scale features\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = self.create_sequences(features_scaled)\n",
    "        \n",
    "        # Train/validation split\n",
    "        train_size = int(len(X) * train_split)\n",
    "        X_train, X_val = X[:train_size], X[train_size:]\n",
    "        y_train, y_val = y[:train_size], y[train_size:]\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss'),\n",
    "            ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6, monitor='val_loss')\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        print(f\"LSTM training complete. Best val_loss: {min(history.history['val_loss']):.6f}\")\n",
    "        return self\n",
    "    \n",
    "    def predict_next_state(self, current_state_dict, current_time=0, sequence_history=None):\n",
    "        \"\"\"Predict next state using LSTM\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not trained!\")\n",
    "        \n",
    "        current_features = state_to_features(\n",
    "            current_state_dict[\"holding_registers\"], \n",
    "            current_state_dict[\"coils\"],\n",
    "            current_time\n",
    "        )\n",
    "        \n",
    "        # Use sequence history if available\n",
    "        if sequence_history is None or len(sequence_history) < self.sequence_length:\n",
    "            sequence = np.tile(current_features, (self.sequence_length, 1))\n",
    "        else:\n",
    "            sequence = np.array(sequence_history[-self.sequence_length:])\n",
    "        \n",
    "        # Scale and predict\n",
    "        sequence_scaled = self.scaler.transform(sequence)\n",
    "        sequence_scaled = sequence_scaled.reshape(1, self.sequence_length, self.feature_dim)\n",
    "        \n",
    "        prediction_scaled = self.model.predict(sequence_scaled, verbose=0)[0]\n",
    "        prediction = self.scaler.inverse_transform(prediction_scaled.reshape(1, -1))[0]\n",
    "        \n",
    "        next_holding_registers, next_coils = features_to_state(prediction)\n",
    "        \n",
    "        return {\"holding_registers\": next_holding_registers, \"coils\": next_coils}\n",
    "\n",
    "class XGBoostStatePredictor:\n",
    "    \"\"\"XGBoost Model\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.scaler = RobustScaler()\n",
    "        self.is_fitted = False\n",
    "        self.feature_dim = 174\n",
    "        \n",
    "    def process_dataset(self, file_path, max_rows=20000):\n",
    "        \"\"\"Process dataset for XGBoost\"\"\"\n",
    "        print(\"Processing dataset for XGBoost...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if len(df) > max_rows:\n",
    "            df = df.head(max_rows)\n",
    "        \n",
    "        holding_registers = {i: {\"value\": 0, \"last_changed\": 0, \"change_count\": 0} for i in range(39)}\n",
    "        coils = {i: {\"value\": 0, \"last_changed\": 0, \"change_count\": 0} for i in range(19)}\n",
    "        \n",
    "        state_sequence = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            current_state_features = state_to_features(holding_registers, coils, idx)\n",
    "            state_sequence.append(current_state_features)\n",
    "            update_state_with_transaction(row, idx, holding_registers, coils)\n",
    "            \n",
    "            if idx % 2000 == 0 and idx > 0:\n",
    "                print(f\"  Processed {idx} transactions...\")\n",
    "        \n",
    "        final_state_features = state_to_features(holding_registers, coils, len(df))\n",
    "        state_sequence.append(final_state_features)\n",
    "        \n",
    "        return np.array(state_sequence)\n",
    "    \n",
    "    def train(self, features, train_split=0.95):\n",
    "        \"\"\"Train XGBoost models\"\"\"\n",
    "        print(f\"Training XGBoost on {features.shape[0]} states...\")\n",
    "        \n",
    "        # Scale features\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Create X, y pairs\n",
    "        X = features_scaled[:-1]\n",
    "        y = features_scaled[1:]\n",
    "        \n",
    "        # Train/test split\n",
    "        train_size = int(len(X) * train_split)\n",
    "        X_train = X[:train_size]\n",
    "        y_train = y[:train_size]\n",
    "        \n",
    "        # XGBoost parameters\n",
    "        xgb_params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 1.0,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'tree_method': 'hist'\n",
    "        }\n",
    "        \n",
    "        # Train separate models for different feature groups\n",
    "        feature_groups = {\n",
    "            'holding_reg_values': list(range(0, 39*3, 3)),\n",
    "            'holding_reg_temporal': list(range(1, 39*3, 3)) + list(range(2, 39*3, 3)),\n",
    "            'coil_values': list(range(39*3, 39*3 + 19*3, 3)),\n",
    "            'coil_temporal': list(range(39*3 + 1, 39*3 + 19*3, 3)) + list(range(39*3 + 2, 39*3 + 19*3, 3))\n",
    "        }\n",
    "        \n",
    "        self.models = {}\n",
    "        \n",
    "        for group_name, feature_indices in feature_groups.items():\n",
    "            print(f\"  Training {group_name} model...\")\n",
    "            \n",
    "            model = xgb.XGBRegressor(**xgb_params)\n",
    "            y_group = y_train[:, feature_indices]\n",
    "            model.fit(X_train, y_group, verbose=False)\n",
    "            \n",
    "            self.models[group_name] = {\n",
    "                'model': model,\n",
    "                'feature_indices': feature_indices\n",
    "            }\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        print(\"XGBoost training complete!\")\n",
    "        return self\n",
    "    \n",
    "    def predict_next_state(self, current_state_dict, current_time=0):\n",
    "        \"\"\"Predict next state using XGBoost\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not trained!\")\n",
    "        \n",
    "        current_features = state_to_features(\n",
    "            current_state_dict[\"holding_registers\"], \n",
    "            current_state_dict[\"coils\"],\n",
    "            current_time\n",
    "        )\n",
    "        \n",
    "        current_features_scaled = self.scaler.transform(np.array(current_features).reshape(1, -1))\n",
    "        \n",
    "        # Predict using each model and combine results\n",
    "        prediction_scaled = np.zeros(self.feature_dim)\n",
    "        \n",
    "        for group_name, model_info in self.models.items():\n",
    "            model = model_info['model']\n",
    "            feature_indices = model_info['feature_indices']\n",
    "            \n",
    "            group_prediction = model.predict(current_features_scaled)[0]\n",
    "            prediction_scaled[feature_indices] = group_prediction\n",
    "        \n",
    "        prediction = self.scaler.inverse_transform(prediction_scaled.reshape(1, -1))[0]\n",
    "        next_holding_registers, next_coils = features_to_state(prediction)\n",
    "        \n",
    "        return {\"holding_registers\": next_holding_registers, \"coils\": next_coils}\n",
    "\n",
    "class RandomForestStatePredictor:\n",
    "    \"\"\"Random Forest Model\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.scaler = RobustScaler()\n",
    "        self.is_fitted = False\n",
    "        self.feature_dim = 174\n",
    "        \n",
    "    def process_dataset(self, file_path, max_rows=20000):\n",
    "        \"\"\"Process dataset for Random Forest\"\"\"\n",
    "        print(\"Processing dataset for Random Forest...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if len(df) > max_rows:\n",
    "            df = df.head(max_rows)\n",
    "        \n",
    "        holding_registers = {i: {\"value\": 0, \"last_changed\": 0, \"change_count\": 0} for i in range(39)}\n",
    "        coils = {i: {\"value\": 0, \"last_changed\": 0, \"change_count\": 0} for i in range(19)}\n",
    "        \n",
    "        state_sequence = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            current_state_features = state_to_features(holding_registers, coils, idx)\n",
    "            state_sequence.append(current_state_features)\n",
    "            update_state_with_transaction(row, idx, holding_registers, coils)\n",
    "            \n",
    "            if idx % 2000 == 0 and idx > 0:\n",
    "                print(f\"  Processed {idx} transactions...\")\n",
    "        \n",
    "        final_state_features = state_to_features(holding_registers, coils, len(df))\n",
    "        state_sequence.append(final_state_features)\n",
    "        \n",
    "        return np.array(state_sequence)\n",
    "    \n",
    "    def train(self, features, train_split=0.95):\n",
    "        \"\"\"Train Random Forest\"\"\"\n",
    "        print(f\"Training Random Forest on {features.shape[0]} states...\")\n",
    "        \n",
    "        # Scale features\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Create X, y pairs\n",
    "        X = features_scaled[:-1]\n",
    "        y = features_scaled[1:]\n",
    "        \n",
    "        # Train/test split\n",
    "        train_size = int(len(X) * train_split)\n",
    "        X_train = X[:train_size]\n",
    "        y_train = y[:train_size]\n",
    "        \n",
    "        # Optimized Random Forest parameters\n",
    "        rf_params = {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 15,\n",
    "            'min_samples_split': 5,\n",
    "            'min_samples_leaf': 2,\n",
    "            'max_features': 'sqrt',\n",
    "            'bootstrap': True,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        self.model = RandomForestRegressor(**rf_params)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        print(\"Random Forest training complete!\")\n",
    "        return self\n",
    "    \n",
    "    def predict_next_state(self, current_state_dict, current_time=0):\n",
    "        \"\"\"Predict next state using Random Forest\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not trained!\")\n",
    "        \n",
    "        current_features = state_to_features(\n",
    "            current_state_dict[\"holding_registers\"], \n",
    "            current_state_dict[\"coils\"],\n",
    "            current_time\n",
    "        )\n",
    "        \n",
    "        current_features_scaled = self.scaler.transform(np.array(current_features).reshape(1, -1))\n",
    "        prediction_scaled = self.model.predict(current_features_scaled)[0]\n",
    "        \n",
    "        prediction = self.scaler.inverse_transform(prediction_scaled.reshape(1, -1))[0]\n",
    "        next_holding_registers, next_coils = features_to_state(prediction)\n",
    "        \n",
    "        return {\"holding_registers\": next_holding_registers, \"coils\": next_coils}\n",
    "\n",
    "# ===== MAIN TRAINING FUNCTION =====\n",
    "\n",
    "def train_and_save_all_models(dataset_path=\"modbus_output_with_time.csv\", \n",
    "                              save_dir=\"models/saved_models\",\n",
    "                              max_rows=20000,\n",
    "                              train_split=0.95):\n",
    "    \"\"\"Train and save all models\"\"\"\n",
    "    \n",
    "    print(\"🚀 Starting Model Training Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Dataset: {dataset_path}\")\n",
    "    print(f\"Max rows: {max_rows}\")\n",
    "    print(f\"Train split: {train_split}\")\n",
    "    print(f\"Save directory: {save_dir}\")\n",
    "    \n",
    "    # Create save directory\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    models_to_train = {\n",
    "        # \"hmm\": HMMStatePredictor(n_components=25),\n",
    "        \"lstm\": LSTMStatePredictor(sequence_length=10, lstm_units=128),\n",
    "        \"xgboost\": XGBoostStatePredictor(),\n",
    "        \"random_forest\": RandomForestStatePredictor()\n",
    "    }\n",
    "    \n",
    "    trained_models = {}\n",
    "    training_times = {}\n",
    "    \n",
    "    for model_name, model in models_to_train.items():\n",
    "        print(f\"\\n{'='*20} Training {model_name.upper()} {'='*20}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Process dataset\n",
    "            features = model.process_dataset(dataset_path, max_rows)\n",
    "            print(f\"Feature shape: {features.shape}\")\n",
    "            \n",
    "            # Train model\n",
    "            trained_model = model.train(features, train_split)\n",
    "            \n",
    "            # Save model\n",
    "            model_path = Path(save_dir) / f\"{model_name}_model.pkl\"\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(trained_model, f)\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            training_times[model_name] = training_time\n",
    "            trained_models[model_name] = trained_model\n",
    "            \n",
    "            print(f\"✅ {model_name.upper()} training complete in {training_time:.2f}s\")\n",
    "            print(f\"💾 Saved to: {model_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {model_name.upper()} training failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Save training summary\n",
    "    summary = {\n",
    "        \"training_times\": training_times,\n",
    "        \"dataset_path\": dataset_path,\n",
    "        \"max_rows\": max_rows,\n",
    "        \"train_split\": train_split,\n",
    "        \"feature_dim\": 174,\n",
    "        \"models_trained\": list(trained_models.keys()),\n",
    "        \"training_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    summary_path = Path(save_dir) / \"training_summary.json\"\n",
    "    import json\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"🎉 MODEL TRAINING COMPLETE!\")\n",
    "    print(f\"📋 Models trained: {', '.join(trained_models.keys())}\")\n",
    "    print(f\"⏱️  Total time: {sum(training_times.values()):.2f}s\")\n",
    "    print(f\"📁 All models saved in: {save_dir}\")\n",
    "    print(f\"📄 Training summary: {summary_path}\")\n",
    "    \n",
    "    return trained_models, summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the training pipeline\n",
    "    trained_models, summary = train_and_save_all_models()\n",
    "    \n",
    "    print(\"\\n🔍 Quick model test...\")\n",
    "    \n",
    "    # Test each model with a simple prediction\n",
    "    test_state = {\n",
    "        \"holding_registers\": {i: 0 for i in range(39)},\n",
    "        \"coils\": {i: 0 for i in range(19)}\n",
    "    }\n",
    "    test_state[\"holding_registers\"][0] = 100\n",
    "    test_state[\"coils\"][0] = 1\n",
    "    \n",
    "    for model_name, model in trained_models.items():\n",
    "        try:\n",
    "            prediction = model.predict_next_state(test_state, current_time=1000)\n",
    "            non_zero_regs = sum(1 for v in prediction[\"holding_registers\"].values() if v != 0)\n",
    "            non_zero_coils = sum(1 for v in prediction[\"coils\"].values() if v != 0)\n",
    "            print(f\"✅ {model_name}: {non_zero_regs} active registers, {non_zero_coils} active coils\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {model_name} test failed: {e}\")\n",
    "    \n",
    "    print(\"\\n✨ Ready to start ensemble system!\")\n",
    "    print(\"Next step: Run individual model notebooks and orchestrator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89c988-c737-4391-8740-8197d4e07064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
